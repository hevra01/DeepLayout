{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_717000/15643851.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(file_path)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# Path to the .pt file\n",
    "dir_path = '/share_chairilg/data/ade20k/'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['coords', 'text', 'context_coords', 'context_text', 'opacity', 'segmentations', 'labels'])\n",
      "27574\n",
      "31\n"
     ]
    }
   ],
   "source": [
    "print(data.keys())\n",
    "print(len(loaded_data))\n",
    "print(len(loaded_data[0]['labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "class Padding(object):\n",
    "    def __init__(self, max_length, vocab_size):\n",
    "        self.max_length = max_length\n",
    "        self.bos_token = vocab_size - 3\n",
    "        self.eos_token = vocab_size - 2\n",
    "        self.pad_token = vocab_size - 1\n",
    "\n",
    "    def __call__(self, layout):\n",
    "        # grab a chunk of (max_length + 1) from the layout\n",
    "\n",
    "        chunk = torch.zeros(self.max_length+1, dtype=torch.long) + self.pad_token\n",
    "        # Assume len(item) will always be <= self.max_length:\n",
    "        chunk[0] = self.bos_token\n",
    "        chunk[1:len(layout)+1] = layout\n",
    "        chunk[len(layout)+1] = self.eos_token\n",
    "\n",
    "        x = chunk[:-1]\n",
    "        y = chunk[1:]\n",
    "        return {'x': x, 'y': y}\n",
    "    \n",
    "class ADE20KDataset(Dataset):\n",
    "    def __init__(self, dir_path, precision=9, max_length=None, standard_frame_height=512, standard_frame_width=512):\n",
    "\n",
    "        raw_dataset = self.load_dataset(self, dir_path)\n",
    "        \n",
    "        # these values will be used to adjust the coordinates to be between \n",
    "        self.standard_frame_height = standard_frame_height\n",
    "        self.standard_frame_width = standard_frame_width\n",
    "        self.standard_full_frame= [0,0,self.standard_frame_width,self.standard_frame_height]\n",
    "\n",
    "        self.to_centerpoints=True,\n",
    "        self.center_xy = True,\n",
    "        self.center_size=False\n",
    "        self.half_size = True\n",
    "\n",
    "        # we give the background as a condition while generating a scene.\n",
    "        # the background will occupy the whole scene. \n",
    "        self.background = torch.tensor([[0, 0, self.standard_frame_width - 1, self.standard_frame_height - 1]])\n",
    "        \n",
    "        # this is for the bounding boxes.\n",
    "        # they will have values between 0 - self.size\n",
    "        self.size = pow(2, precision)\n",
    "\n",
    "        # find the categories present in the data\n",
    "        self.categories = self.get_categories(raw_dataset)\n",
    "        \n",
    "        # we are reserving vocab from 0 to self.size - 1 for the bounding boxes,\n",
    "        # hence we need to shift the categories by self.size\n",
    "        self.shift_by_self_size = {\n",
    "            v: i + self.size for i, v in enumerate(self.categories.values())\n",
    "        }\n",
    "        \n",
    "        self.undo_shift = {\n",
    "            v: k for k, v in self.shift_by_self_size.items()\n",
    "        }\n",
    "\n",
    "        self.vocab_size = self.size + len(self.categories) + 3  # bos, eos, pad tokens\n",
    "        self.bos_token = self.vocab_size - 3\n",
    "        self.eos_token = self.vocab_size - 2\n",
    "        self.pad_token = self.vocab_size - 1\n",
    "\n",
    "        # this should hold a flattened list of [category_id, x, y, w, h]\n",
    "        self.data = []\n",
    "\n",
    "        # iterate over the data \n",
    "        for image in raw_dataset:\n",
    "\n",
    "            # get the string labels\n",
    "            labels = image['labels']\n",
    "\n",
    "            # convert the string labels to integer labels\n",
    "            integer_labels = [self.categories[label] for label in labels]\n",
    "\n",
    "            # shift the integer labels by self.size\n",
    "            shifted_labels = np.expand_dims(np.array([self.shift_by_self_size[label] for label in integer_labels]), axis=1)\n",
    "            \n",
    "            # get the raw coordinates\n",
    "            coordinates = image['coords']\n",
    "\n",
    "            # convert the raw coordinates to the standard frame\n",
    "            converted_coords = self.convert_tensor_to_standard(coordinates[:len(labels)-1, :])\n",
    "\n",
    "            # append the background to the converted coordinates\n",
    "            converted_coords = torch.cat((self.background, converted_coords), dim=0)\n",
    "\n",
    "            # Sort boxes\n",
    "            ann_box = np.array(converted_coords)\n",
    "            ind = np.lexsort((ann_box[:, 0], ann_box[:, 1]))\n",
    "            ann_box = ann_box[ind]\n",
    "            \n",
    "            # Append the categories to the coordinates\n",
    "            layout = np.concatenate([shifted_labels, ann_box], axis=1)\n",
    "            \n",
    "            # Flatten and add to the dataset\n",
    "            self.data.append(layout.reshape(-1))\n",
    "\n",
    "\n",
    "        self.max_length = max_length\n",
    "        if self.max_length is None:\n",
    "            self.max_length = max([len(x) for x in self.data]) + 2  # bos, eos tokens\n",
    "        self.transform = Padding(self.max_length, self.vocab_size)\n",
    "\n",
    "    def load_dataset(self, dir_path):\n",
    "        # first, load the dataset given the path\n",
    "        # Initialize a list to store the loaded data\n",
    "        loaded_data = []\n",
    "\n",
    "        # List all .pt files (assuming the format is '00000.pt' to '27573.pt')\n",
    "        num_files = 27574  # Total number of files\n",
    "        for i in range(num_files):\n",
    "            filename = f'{str(i).zfill(5)}.pt'  # Create the filename, e.g., '00000.pt', '00001.pt'\n",
    "            file_path = os.path.join(dir_path, filename)\n",
    "            \n",
    "            # Check if the file exists\n",
    "            if os.path.exists(file_path):\n",
    "                \n",
    "                # Load the .pt file\n",
    "                data = torch.load(file_path)\n",
    "\n",
    "                # Append the loaded data to the list\n",
    "                loaded_data.append(data)\n",
    "        return loaded_data\n",
    "        \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # grab a chunk of (block_size + 1) tokens from the data\n",
    "        layout = torch.tensor(self.data[idx], dtype=torch.long)\n",
    "        layout = self.transform(layout)\n",
    "        return layout['x'], layout['y']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def get_categories(self, raw_dataset):\n",
    "        # Initialize a dictionary where keys are categories, and values are indices.\n",
    "        categories = {}\n",
    "        current_index = 0\n",
    "        \n",
    "        for d in raw_dataset:\n",
    "            labels = d['labels']\n",
    "            for label in labels:\n",
    "                # If the label is not already in the dictionary, add it with the next index\n",
    "                if label not in categories:\n",
    "                    categories[label] = current_index\n",
    "                    current_index += 1\n",
    "        \n",
    "        return categories\n",
    "    \n",
    "    def convert_tensor_to_standard(self, coords_tensor):\n",
    "        # Assuming coords_tensor has shape (N, 4) where N is the number of objects\n",
    "        x_coord, y_coord, width, height = coords_tensor[:, 0], coords_tensor[:, 1], coords_tensor[:, 2], coords_tensor[:, 3]\n",
    "\n",
    "        if self.half_size:\n",
    "            width = width * 2\n",
    "            height = height * 2\n",
    "\n",
    "        if self.center_size:\n",
    "            width = width + 0.5\n",
    "            height = height + 0.5\n",
    "\n",
    "        if self.center_xy:\n",
    "            x_coord = x_coord + 0.5\n",
    "            y_coord = y_coord + 0.5\n",
    "\n",
    "        if self.to_centerpoints:\n",
    "            x_min = x_coord - (width / 2)\n",
    "            y_min = y_coord - (height / 2)\n",
    "        else:\n",
    "            x_min = x_coord\n",
    "            y_min = y_coord\n",
    "\n",
    "        # Scale back to standard frame dimensions (e.g., 512x512)\n",
    "        x_min = (x_min * self.standard_frame_width).int()\n",
    "        y_min = (y_min * self.standard_frame_height).int()\n",
    "        width = (width * self.standard_frame_width).int()\n",
    "        height = (height * self.standard_frame_height).int()\n",
    "\n",
    "        # Ensure minimum width and height\n",
    "        width = torch.clamp(width, min=1)\n",
    "        height = torch.clamp(height, min=1)\n",
    "\n",
    "        return torch.stack([x_min, y_min, width, height], dim=1)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_717000/149188683.py:89: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n",
      "  ann_box = np.array(converted_coords)\n"
     ]
    }
   ],
   "source": [
    "dataset = ADE20KDataset(loaded_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "layout",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
